---
# Source: graylog/charts/elasticsearch/templates/client-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.32.2
    component: "client"
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-elasticsearch-client
---
# Source: graylog/charts/elasticsearch/templates/data-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.32.2
    component: "data"
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-elasticsearch-data
---
# Source: graylog/charts/elasticsearch/templates/master-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.32.2
    component: "master"
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-elasticsearch-master
---
# Source: graylog/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: R31E@S3-graylog
  labels:
    helm.sh/chart: graylog-1.6.10
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.1"
---
# Source: graylog/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: R31E@S3-graylog
  labels:
    helm.sh/chart: graylog-1.6.10
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.1"
type: Opaque
data:
  graylog-root-username: "Gr@yL0gR00tUs3rN@m3"
  graylog-password-secret: "Gr@yL0gP@ssw0rdS3cr3t"
  graylog-password-sha2: "Gr@yL0gP@ssw0rdSh@"
---
# Source: graylog/charts/elasticsearch/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: R31E@S3-elasticsearch
  labels:
    app: R31E@S3-elasticsearch
    chart: "elasticsearch-1.32.2"
    release: "R31E@S3"
    heritage: "Helm"
data:
  elasticsearch.yml: |-
    cluster.name: elasticsearch

    node.data: ${NODE_DATA:true}
    node.master: ${NODE_MASTER:true}
    node.ingest: ${NODE_INGEST:true}
    node.name: ${HOSTNAME}
    network.host: 0.0.0.0
    # see https://github.com/kubernetes/kubernetes/issues/3595
    bootstrap.memory_lock: ${BOOTSTRAP_MEMORY_LOCK:false}

    discovery:
      zen:
        ping.unicast.hosts: ${DISCOVERY_SERVICE:}
        minimum_master_nodes: ${MINIMUM_MASTER_NODES:2}

    # see https://github.com/elastic/elasticsearch-definitive-guide/pull/679
    processors: ${PROCESSORS:}

    # avoid split-brain w/ a minimum consensus of two masters plus a data node
    gateway.expected_master_nodes: ${EXPECTED_MASTER_NODES:2}
    gateway.expected_data_nodes: ${EXPECTED_DATA_NODES:1}
    gateway.recover_after_time: ${RECOVER_AFTER_TIME:5m}
    gateway.recover_after_master_nodes: ${RECOVER_AFTER_MASTER_NODES:2}
    gateway.recover_after_data_nodes: ${RECOVER_AFTER_DATA_NODES:1}
  log4j2.properties: |-
    status = error
    appender.console.type = Console
    appender.console.name = console
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] %marker%m%n
    rootLogger.level = info
    rootLogger.appenderRef.console.ref = console
    logger.searchguard.name = com.floragunn
    logger.searchguard.level = info
    
  data-pre-stop-hook.sh: |-
    #!/bin/bash
    exec &> >(tee -a "/var/log/elasticsearch-hooks.log")
    NODE_NAME=${HOSTNAME}
    echo "Prepare to migrate data of the node ${NODE_NAME}"
    echo "Move all data from node ${NODE_NAME}"
    curl -s -XPUT -H 'Content-Type: application/json' 'R31E@S3-elasticsearch-client:9200/_cluster/settings' -d "{
      \"transient\" :{
          \"cluster.routing.allocation.exclude._name\" : \"${NODE_NAME}\"
      }
    }"
    echo ""

    while true ; do
      echo -e "Wait for node ${NODE_NAME} to become empty"
      SHARDS_ALLOCATION=$(curl -s -XGET 'http://R31E@S3-elasticsearch-client:9200/_cat/shards')
      if ! echo "${SHARDS_ALLOCATION}" | grep -E "${NODE_NAME}"; then
        break
      fi
      sleep 1
    done
    echo "Node ${NODE_NAME} is ready to shutdown"
  data-post-start-hook.sh: |-
    #!/bin/bash
    exec &> >(tee -a "/var/log/elasticsearch-hooks.log")
    NODE_NAME=${HOSTNAME}
    CLUSTER_SETTINGS=$(curl -s -XGET "http://R31E@S3-elasticsearch-client:9200/_cluster/settings")
    if echo "${CLUSTER_SETTINGS}" | grep -E "${NODE_NAME}"; then
      echo "Activate node ${NODE_NAME}"
      curl -s -XPUT -H 'Content-Type: application/json' "http://R31E@S3-elasticsearch-client:9200/_cluster/settings" -d "{
        \"transient\" :{
            \"cluster.routing.allocation.exclude._name\" : null
        }
      }"
    fi
    echo "Node ${NODE_NAME} is ready to be used"
---
# Source: graylog/charts/elasticsearch/templates/tests/test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: R31E@S3-elasticsearch-test
  labels:
    app: R31E@S3-elasticsearch
    chart: "elasticsearch-1.32.2"
    heritage: "Helm"
    release: "R31E@S3"
data:
  run.sh: |-
    @test "Test Access and Health" {
      curl -D - http://R31E@S3-elasticsearch-client:9200
      curl -D - http://R31E@S3-elasticsearch-client:9200/_cluster/health?wait_for_status=green
    }
---
# Source: graylog/charts/mongodb-replicaset/templates/mongodb-init-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: mongodb-replicaset
    chart: mongodb-replicaset-3.11.2
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-mongodb-replicaset-init
data:
  on-start.sh: |
    #!/usr/bin/env bash
    
    # Copyright 2018 The Kubernetes Authors. All rights reserved.
    #
    # Licensed under the Apache License, Version 2.0 (the "License");
    # you may not use this file except in compliance with the License.
    # You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    
    set -e pipefail
    
    port=27017
    replica_set="$REPLICA_SET"
    script_name=${0##*/}
    SECONDS=0
    timeout="${TIMEOUT:-900}"
    tls_mode="${TLS_MODE}"
    
    if [[ "$AUTH" == "true" ]]; then
        admin_user="$ADMIN_USER"
        admin_password="$ADMIN_PASSWORD"
        admin_creds=(-u "$admin_user" -p "$admin_password")
        if [[ "$METRICS" == "true" ]]; then
            metrics_user="$METRICS_USER"
            metrics_password="$METRICS_PASSWORD"
        fi
        auth_args=("--auth" "--keyFile=/data/configdb/key.txt")
    fi
    
    log() {
        local msg="$1"
        local timestamp
        timestamp=$(date --iso-8601=ns)
        echo "[$timestamp] [$script_name] $msg" 2>&1 | tee -a /work-dir/log.txt 1>&2
    }
    
    retry_until() {
        local host="${1}"
        local command="${2}"
        local expected="${3}"
        local creds=("${admin_creds[@]}")
    
        # Don't need credentials for admin user creation and pings that run on localhost
        if [[ "${host}" =~ ^localhost ]]; then
            creds=()
        fi
    
        until [[ $(mongo admin --host "${host}" "${creds[@]}" "${ssl_args[@]}" --quiet --eval "${command}" | tail -n1) == "${expected}" ]]; do
            sleep 1
    
            if (! ps "${pid}" &>/dev/null); then
                log "mongod shutdown unexpectedly"
                exit 1
            fi
            if [[ "${SECONDS}" -ge "${timeout}" ]]; then
                log "Timed out after ${timeout}s attempting to bootstrap mongod"
                exit 1
            fi
    
            log "Retrying ${command} on ${host}"
        done
    }
    
    shutdown_mongo() {
        local host="${1:-localhost}"
        local args='force: true'
        log "Shutting down MongoDB ($args)..."
        if (! mongo admin --host "${host}" "${admin_creds[@]}" "${ssl_args[@]}" --eval "db.shutdownServer({$args})"); then
          log "db.shutdownServer() failed, sending the terminate signal"
          kill -TERM "${pid}"
        fi
    }
    
    init_mongod_standalone() {
        if [[ ! -f /init/initMongodStandalone.js ]]; then
            log "Skipping init mongod standalone script"
            return 0
        elif [[ -z "$(ls -1A /data/db)" ]]; then
            log "mongod standalone script currently not supported on initial install"
            return 0
        fi
    
        local port="27018"
        log "Starting a MongoDB instance as standalone..."
        mongod --config /data/configdb/mongod.conf --dbpath=/data/db "${auth_args[@]}" "${ssl_server_args[@]}" --port "${port}" --bind_ip=0.0.0.0 2>&1 | tee -a /work-dir/log.txt 1>&2 &
        export pid=$!
        trap shutdown_mongo EXIT
        log "Waiting for MongoDB to be ready..."
        retry_until "localhost:${port}" "db.adminCommand('ping').ok" "1"
        log "Running init js script on standalone mongod"
        mongo admin --port "${port}" "${admin_creds[@]}" "${ssl_args[@]}" /init/initMongodStandalone.js
        shutdown_mongo "localhost:${port}"
    }
    
    my_hostname=$(hostname)
    log "Bootstrapping MongoDB replica set member: $my_hostname"
    
    log "Reading standard input..."
    while read -ra line; do
        if [[ "${line}" == *"${my_hostname}"* ]]; then
            service_name="$line"
        fi
        peers=("${peers[@]}" "$line")
    done
    
    # Generate the ca cert
    ca_crt=/data/configdb/tls.crt
    if [ -f "$ca_crt"  ]; then
        log "Generating certificate"
        ca_key=/data/configdb/tls.key
        pem=/work-dir/mongo.pem
        ssl_args=(--ssl --sslCAFile "$ca_crt" --sslPEMKeyFile "$pem")
        ssl_server_args=(--sslMode "$tls_mode" --sslCAFile "$ca_crt" --sslPEMKeyFile "$pem")
    
    # Move into /work-dir
    pushd /work-dir
    
    cat >openssl.cnf <<EOL
    [req]
    req_extensions = v3_req
    distinguished_name = req_distinguished_name
    [req_distinguished_name]
    [ v3_req ]
    basicConstraints = CA:FALSE
    keyUsage = nonRepudiation, digitalSignature, keyEncipherment
    subjectAltName = @alt_names
    [alt_names]
    DNS.1 = $(echo -n "$my_hostname" | sed s/-[0-9]*$//)
    DNS.2 = $my_hostname
    DNS.3 = $service_name
    DNS.4 = localhost
    DNS.5 = 127.0.0.1
    EOL
    
        # Generate the certs
        openssl genrsa -out mongo.key 2048
        openssl req -new -key mongo.key -out mongo.csr -subj "/OU=MongoDB/CN=$my_hostname" -config openssl.cnf
        openssl x509 -req -in mongo.csr \
            -CA "$ca_crt" -CAkey "$ca_key" -CAcreateserial \
            -out mongo.crt -days 3650 -extensions v3_req -extfile openssl.cnf
    
        rm mongo.csr
        cat mongo.crt mongo.key > $pem
        rm mongo.key mongo.crt
    fi
    
    init_mongod_standalone
    
    if [[ "${SKIP_INIT}" == "true" ]]; then
        log "Skipping initialization"
        exit 0
    fi
    
    log "Peers: ${peers[*]}"
    log "Starting a MongoDB replica"
    mongod --config /data/configdb/mongod.conf --dbpath=/data/db --replSet="$replica_set" --port="${port}" "${auth_args[@]}" "${ssl_server_args[@]}" --bind_ip=0.0.0.0 2>&1 | tee -a /work-dir/log.txt 1>&2 &
    pid=$!
    trap shutdown_mongo EXIT
    
    log "Waiting for MongoDB to be ready..."
    retry_until "localhost" "db.adminCommand('ping').ok" "1"
    log "Initialized."
    
    # try to find a master
    for peer in "${peers[@]}"; do
        log "Checking if ${peer} is primary"
        # Check rs.status() first since it could be in primary catch up mode which db.isMaster() doesn't show
        if [[ $(mongo admin --host "${peer}" "${admin_creds[@]}" "${ssl_args[@]}" --quiet --eval "rs.status().myState") == "1" ]]; then
            retry_until "${peer}" "db.isMaster().ismaster" "true"
            log "Found primary: ${peer}"
            primary="${peer}"
            break
        fi
    done
    
    if [[ "${primary}" = "${service_name}" ]]; then
        log "This replica is already PRIMARY"
    elif [[ -n "${primary}" ]]; then
        if [[ $(mongo admin --host "${primary}" "${admin_creds[@]}" "${ssl_args[@]}" --quiet --eval "rs.conf().members.findIndex(m => m.host == '${service_name}:${port}')") == "-1" ]]; then
          log "Adding myself (${service_name}) to replica set..."
          if (mongo admin --host "${primary}" "${admin_creds[@]}" "${ssl_args[@]}" --eval "rs.add('${service_name}')" | grep 'Quorum check failed'); then
              log 'Quorum check failed, unable to join replicaset. Exiting prematurely.'
              exit 1
          fi
        fi
    
        sleep 3
        log 'Waiting for replica to reach SECONDARY state...'
        retry_until "${service_name}" "rs.status().myState" "2"
        log '✓ Replica reached SECONDARY state.'
    
    elif (mongo "${ssl_args[@]}" --eval "rs.status()" | grep "no replset config has been received"); then
        log "Initiating a new replica set with myself ($service_name)..."
        mongo "${ssl_args[@]}" --eval "rs.initiate({'_id': '$replica_set', 'members': [{'_id': 0, 'host': '$service_name'}]})"
    
        sleep 3
        log 'Waiting for replica to reach PRIMARY state...'
        retry_until "localhost" "db.isMaster().ismaster" "true"
        primary="${service_name}"
        log '✓ Replica reached PRIMARY state.'
    
        if [[ "${AUTH}" == "true" ]]; then
            log "Creating admin user..."
            mongo admin "${ssl_args[@]}" --eval "db.createUser({user: '${admin_user}', pwd: '${admin_password}', roles: [{role: 'root', db: 'admin'}]})"
        fi
    fi
    
    # User creation
    if [[ -n "${primary}" && "$AUTH" == "true" && "$METRICS" == "true" ]]; then
        metric_user_count=$(mongo admin --host "${primary}" "${admin_creds[@]}" "${ssl_args[@]}" --eval "db.system.users.find({user: '${metrics_user}'}).count()" --quiet)
        if [[ "${metric_user_count}" == "0" ]]; then
            log "Creating clusterMonitor user..."
            mongo admin --host "${primary}" "${admin_creds[@]}" "${ssl_args[@]}" --eval "db.createUser({user: '${metrics_user}', pwd: '${metrics_password}', roles: [{role: 'clusterMonitor', db: 'admin'}, {role: 'read', db: 'local'}]})"
        fi
    fi
    
    log "MongoDB bootstrap complete"
    exit 0
---
# Source: graylog/charts/mongodb-replicaset/templates/mongodb-mongodb-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: mongodb-replicaset
    chart: mongodb-replicaset-3.11.2
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-mongodb-replicaset-mongodb
data:
  mongod.conf: |
    {}
---
# Source: graylog/charts/mongodb-replicaset/templates/tests/mongodb-up-test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: mongodb-replicaset
    chart: mongodb-replicaset-3.11.2
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-mongodb-replicaset-tests
data:
  mongodb-up-test.sh: |
    #!/usr/bin/env bash
    
    set -ex
    
    CACRT_FILE=/work-dir/tls.crt
    CAKEY_FILE=/work-dir/tls.key
    MONGOPEM=/work-dir/mongo.pem
    
    MONGOARGS="--quiet"
    
    if [ -e "/tls/tls.crt" ]; then
        # log "Generating certificate"
        mkdir -p /work-dir
        cp /tls/tls.crt /work-dir/tls.crt
        cp /tls/tls.key /work-dir/tls.key
    
        # Move into /work-dir
        pushd /work-dir
    
    cat >openssl.cnf <<EOL
    [req]
    req_extensions = v3_req
    distinguished_name = req_distinguished_name
    [req_distinguished_name]
    [ v3_req ]
    basicConstraints = CA:FALSE
    keyUsage = nonRepudiation, digitalSignature, keyEncipherment
    subjectAltName = @alt_names
    [alt_names]
    DNS.1 = $(echo -n "$(hostname)" | sed s/-[0-9]*$//)
    DNS.2 = $(hostname)
    DNS.3 = localhost
    DNS.4 = 127.0.0.1
    EOL
    
        # Generate the certs
        openssl genrsa -out mongo.key 2048
        openssl req -new -key mongo.key -out mongo.csr -subj "/OU=MongoDB/CN=$(hostname)" -config openssl.cnf
        openssl x509 -req -in mongo.csr \
            -CA "$CACRT_FILE" -CAkey "$CAKEY_FILE" -CAcreateserial \
            -out mongo.crt -days 3650 -extensions v3_req -extfile openssl.cnf
        cat mongo.crt mongo.key > $MONGOPEM
        MONGOARGS="$MONGOARGS --ssl --sslCAFile $CACRT_FILE --sslPEMKeyFile $MONGOPEM"
    fi
    
    if [[ "${AUTH}" == "true" ]]; then
        MONGOARGS="$MONGOARGS --username $ADMIN_USER --password $ADMIN_PASSWORD --authenticationDatabase admin"
    fi
    
    pod_name() {
        local full_name="${FULL_NAME?Environment variable FULL_NAME not set}"
        local namespace="${NAMESPACE?Environment variable NAMESPACE not set}"
        local index="$1"
        echo "$full_name-$index.$full_name.$namespace.svc.cluster.local"
    }
    
    replicas() {
        echo "${REPLICAS?Environment variable REPLICAS not set}"
    }
    
    master_pod() {
        for ((i = 0; i < $(replicas); ++i)); do
            response=$(mongo $MONGOARGS "--host=$(pod_name "$i")" "--eval=rs.isMaster().ismaster")
            if [[ "$response" == "true" ]]; then
                pod_name "$i"
                break
            fi
        done
    }
    
    setup() {
        local ready=0
        until [[ "$ready" -eq $(replicas) ]]; do
            echo "Waiting for application to become ready" >&2
            sleep 1
    
            for ((i = 0; i < $(replicas); ++i)); do
                response=$(mongo $MONGOARGS "--host=$(pod_name "$i")" "--eval=rs.status().ok" || true)
                if [[ "$response" -eq 1 ]]; then
                    ready=$((ready + 1))
                fi
            done
        done
    }
    
    @test "Testing mongodb client is executable" {
        mongo -h
        [ "$?" -eq 0 ]
    }
    
    @test "Connect mongodb client to mongodb pods" {
        for ((i = 0; i < $(replicas); ++i)); do
            response=$(mongo $MONGOARGS "--host=$(pod_name "$i")" "--eval=rs.status().ok")
            if [[ ! "$response" -eq 1 ]]; then
                exit 1
            fi
        done
    }
    
    @test "Write key to primary" {
        response=$(mongo $MONGOARGS --host=$(master_pod) "--eval=db.test.insert({\"abc\": \"def\"}).nInserted")
        if [[ ! "$response" -eq 1 ]]; then
            exit 1
        fi
    }
    
    @test "Read key from slaves" {
        # wait for slaves to catch up
        sleep 10
    
        for ((i = 0; i < $(replicas); ++i)); do
            response=$(mongo $MONGOARGS --host=$(pod_name "$i") "--eval=rs.slaveOk(); db.test.find({\"abc\":\"def\"})")
            if [[ ! "$response" =~ .*def.* ]]; then
                exit 1
            fi
        done
    
        # Clean up a document after test
        mongo $MONGOARGS --host=$(master_pod) "--eval=db.test.deleteMany({\"abc\": \"def\"})"
    }
---
# Source: graylog/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: R31E@S3-graylog
  labels:
    helm.sh/chart: graylog-1.6.10
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.1"
data:
  log4j2.xml: |-
    <?xml version="1.0" encoding="UTF-8"?>
    <Configuration packages="org.graylog2.log4j" shutdownHook="disable">
        <Appenders>
            <Console name="STDOUT" target="SYSTEM_OUT">
                <PatternLayout pattern="%d %-7level [%c{1}] - %m - %X%n"/>
            </Console>
            <RollingFile name="rolling-file" fileName="/usr/share/graylog/log/server.log" filePattern="/usr/share/graylog/log/server.log.%i.gz">
                <PatternLayout>
                    <Pattern>%d{yyyy-MM-dd'T'HH:mm:ss.SSSXXX} %-5p [%c{1}] %m%n</Pattern>
                </PatternLayout>
                <Policies>
                    <SizeBasedTriggeringPolicy size="50MB"/>
                </Policies>
                <DefaultRolloverStrategy max="10" fileIndex="min"/>
            </RollingFile>
            <!-- Internal Graylog log appender. Please do not disable. This makes internal log messages available via REST calls. -->
            <Memory name="graylog-internal-logs" bufferSize="500"/>
            <!-- Rotate audit logs daily -->
            <RollingFile name="AUDITLOG" fileName="/usr/share/graylog/log/audit.log" filePattern="/usr/share/graylog/log/audit-%d{yyyy-MM-dd}.log.gz">
                <PatternLayout>
                    <Pattern>%d [%c{1}] - %m - %X%n</Pattern>
                </PatternLayout>
                <Policies>
                    <TimeBasedTriggeringPolicy />
                </Policies>
            </RollingFile>
        </Appenders>
        <Loggers>
            <!-- Application Loggers -->
            <Logger name="org.graylog2" level="warn"/>
            <Logger name="com.github.joschi.jadconfig" level="warn"/>
            <!-- This emits a harmless warning for ActiveDirectory every time which we can't work around :( -->
            <Logger name="org.apache.directory.api.ldap.model.message.BindRequestImpl" level="error"/>
            <!-- Prevent DEBUG message about Lucene Expressions not found. -->
            <Logger name="org.elasticsearch.script" level="warn"/>
            <!-- Disable messages from the version check -->
            <Logger name="org.graylog2.periodical.VersionCheckThread" level="off"/>
            <!-- Suppress crazy byte array dump of Drools -->
            <Logger name="org.drools.compiler.kie.builder.impl.KieRepositoryImpl" level="warn"/>
            <!-- Silence chatty natty -->
            <Logger name="com.joestelmach.natty.Parser" level="warn"/>
            <!-- Silence Kafka log chatter -->
            <Logger name="kafka.log.Log" level="warn"/>
            <Logger name="kafka.log.OffsetIndex" level="warn"/>
            <!-- Silence useless session validation messages -->
            <Logger name="org.apache.shiro.session.mgt.AbstractValidatingSessionManager" level="warn"/>
            <Root level="warn">
                <AppenderRef ref="STDOUT"/>
            </Root>
            <!-- Security Loggers -->
          <Logger name="org.graylog2.security.realm.PasswordAuthenticator" level="trace" additivity="false">
                <AppenderRef ref="AUDITLOG"/>
            </Logger>
            <Logger name="org.graylog2.security.realm.AccessTokenAuthenticator" level="trace" additivity="false">
                <AppenderRef ref="AUDITLOG"/>
            </Logger>
            <Logger name="org.graylog2.security.realm.RootAccountRealm" level="trace" additivity="false">
                <AppenderRef ref="AUDITLOG"/>
            </Logger>
            <Logger name="org.graylog2.shared.security.ShiroAuthorizationFilter" level="trace" additivity="false">
                <AppenderRef ref="AUDITLOG"/>
            </Logger>
        </Loggers>
    </Configuration>
  graylog.conf: |-
    node_id_file = /usr/share/graylog/data/journal/node-id
    root_username = admin
    root_email = 
    root_timezone = UTC
    plugin_dir = /usr/share/graylog/plugin
    http_bind_address = 0.0.0.0:9000
    http_external_uri = Gr@ylogExt3rn@lUR1/
    elasticsearch_hosts = http://R31E@S3-elasticsearch-client.N@m3Sp@c3.svc.cluster.local:9200
    allow_leading_wildcard_searches = false
    allow_highlighting = false
    output_batch_size = 500
    output_flush_interval = 1
    output_fault_count_threshold = 5
    output_fault_penalty_seconds = 30
    processbuffer_processors = 5
    outputbuffer_processors = 3
    processor_wait_strategy = blocking
    ring_size = 65536
    inputbuffer_ring_size = 65536
    inputbuffer_processors = 2
    inputbuffer_wait_strategy = blocking
    message_journal_enabled = true
    # Do not change `message_journal_dir` location
    message_journal_dir = /usr/share/graylog/data/journal
    lb_recognition_period_seconds = 3
    # Use a replica set instead of a single host
    mongodb_uri = mongodb://R31E@S3-mongodb-replicaset.N@m3Sp@c3.svc.cluster.local:27017/graylog?replicaSet=rs0
    mongodb_max_connections = 1000
    mongodb_threads_allowed_to_block_multiplier = 5
    # Email transport
    transport_email_enabled = false
    transport_email_hostname = 
    transport_email_port = 2587
    transport_email_use_auth = true
    transport_email_use_tls = true
    transport_email_use_ssl = false
    transport_email_auth_username = 
    transport_email_auth_password = 
    transport_email_subject_prefix = [graylog]
    transport_email_from_email = 
    transport_email_web_interface_url = Gr@yL0gEmailWebInterfaceURl
    content_packs_dir = /usr/share/graylog/data/contentpacks
    content_packs_auto_load = grok-patterns.json
    proxied_requests_thread_pool_size = 32
  entrypoint.sh: |-
    #!/usr/bin/env bash

    export GRAYLOG_HTTP_PUBLISH_URI="http://$(hostname -f):9000/"

    GRAYLOG_HOME=/usr/share/graylog
    # Looking for Master IP
    MASTER_IP=`/k8s/kubectl --namespace N@m3Sp@c3 get pod -o jsonpath='{range .items[*]}{.metadata.name} {.status.podIP}{"\n"}{end}' -l graylog-role=master --field-selector=status.phase=Running|awk '{print $2}'`
    SELF_IP=`/k8s/kubectl --namespace N@m3Sp@c3 get pod $HOSTNAME -o jsonpath='{.status.podIP}'`
    echo "Current master is $MASTER_IP"
    echo "Self IP is $SELF_IP"
    if [[ -z "$MASTER_IP" ]]; then
      echo "Launching $HOSTNAME as master"
      export GRAYLOG_IS_MASTER="true"
      /k8s/kubectl --namespace N@m3Sp@c3 label --overwrite pod $HOSTNAME graylog-role="master"
    else
      # When container was recreated or restart, MASTER_IP == SELF_IP, running as master and no need to change label graylog-role="master"
      if [ "$SELF_IP" == "$MASTER_IP" ];then
        export GRAYLOG_IS_MASTER="true"
      else
        # MASTER_IP != SELF_IP, running as coordinating
        echo "Launching $HOSTNAME as coordinating"
        export GRAYLOG_IS_MASTER="false"
        /k8s/kubectl --namespace N@m3Sp@c3 label --overwrite pod $HOSTNAME graylog-role="coordinating"
      fi
    fi
    # Download plugins
    echo "Downloading https://github.com/graylog-labs/graylog-plugin-metrics-reporter/releases/download/3.0.0/metrics-reporter-prometheus-3.0.0.jar ..."
    curl -s --location --retry 3 -o ${GRAYLOG_HOME}/plugin/metrics-reporter-prometheus-3.0.0.jar "https://github.com/graylog-labs/graylog-plugin-metrics-reporter/releases/download/3.0.0/metrics-reporter-prometheus-3.0.0.jar"
    # Start Graylog
    echo "Starting graylog"
    # Original docker-entrypoint.sh in Graylog Docker will error while executing since you can't chown readonly files in `config`
    # exec /docker-entrypoint.sh graylog
    echo "Graylog Home ${GRAYLOG_HOME}"
    echo "JVM Options ${GRAYLOG_SERVER_JAVA_OPTS}"
    "${JAVA_HOME}/bin/java" \
      ${GRAYLOG_SERVER_JAVA_OPTS} \
      -jar \
      -Dlog4j.configurationFile=${GRAYLOG_HOME}/config/log4j2.xml \
      -Djava.library.path=${GRAYLOG_HOME}/lib/sigar/ \
      -Dgraylog2.installation_source=docker \
      ${GRAYLOG_HOME}/graylog.jar \
      server \
      -f ${GRAYLOG_HOME}/config/graylog.conf
---
# Source: graylog/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: R31E@S3-graylog
  labels:
    app.kubernetes.io/name: graylog
    helm.sh/chart: graylog-1.6.10
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "R31E@S3"
    app.kubernetes.io/version: "3.1"
rules:
- apiGroups:
    - ""
  resources:
    - pods
    - secrets
  verbs:
    - get
    - list
    - patch
---
# Source: graylog/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: R31E@S3-graylog
  labels:
    helm.sh/chart: graylog-1.6.10
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.1"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: R31E@S3-graylog
subjects:
- kind: ServiceAccount
  name: R31E@S3-graylog
---
# Source: graylog/charts/elasticsearch/templates/client-svc.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.32.2
    component: "client"
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-elasticsearch-client

spec:
  ports:
    - name: http
      port: 9200
      targetPort: http
  selector:
    app: elasticsearch
    component: "client"
    release: R31E@S3
  type: ClusterIP
---
# Source: graylog/charts/elasticsearch/templates/master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.32.2
    component: "master"
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-elasticsearch-discovery
spec:
  clusterIP: None
  ports:
    - port: 9300
      targetPort: transport
  selector:
    app: elasticsearch
    component: "master"
    release: R31E@S3
---
# Source: graylog/charts/mongodb-replicaset/templates/mongodb-service-client.yaml
# A headless service for client applications to use
apiVersion: v1
kind: Service
metadata:
  annotations:
  labels:
    app: mongodb-replicaset
    chart: mongodb-replicaset-3.11.2
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-mongodb-replicaset-client
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: mongodb
      port: 27017
  selector:
    app: mongodb-replicaset
    release: R31E@S3
---
# Source: graylog/charts/mongodb-replicaset/templates/mongodb-service.yaml
# A headless service to create DNS records for discovery purposes. Use the -client service to connect applications
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  labels:
    app: mongodb-replicaset
    chart: mongodb-replicaset-3.11.2
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-mongodb-replicaset
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: mongodb
      port: 27017
  publishNotReadyAddresses: true
  selector:
    app: mongodb-replicaset
    release: R31E@S3
---
# Source: graylog/templates/headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: R31E@S3-graylog
  labels:
    helm.sh/chart: graylog-1.6.10
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.1"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
    - name: graylog
      port: 9000
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
---
# Source: graylog/templates/master-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: R31E@S3-graylog-master
  labels:
    helm.sh/chart: graylog-1.6.10
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.1"
    graylog-role: "master"
spec:
  ports:
    - name: graylog
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
    graylog-role: "master"
  type: "ClusterIP"
---
# Source: graylog/templates/web-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: R31E@S3-graylog-web
  labels:
    helm.sh/chart: graylog-1.6.10
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.1"
    app.kubernetes.io/component: "web"
spec:
  ports:
    - name: graylog
      port: 9000
      protocol: TCP
      targetPort: 9000
  selector:
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
  type: "ClusterIP"
---
# Source: graylog/charts/elasticsearch/templates/client-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.32.2
    component: "client"
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-elasticsearch-client
spec:
  selector:
    matchLabels:
      app: elasticsearch
      component: "client"
      release: R31E@S3
  replicas: e1@st1cC1i3ntR3p1ic@
  template:
    metadata:
      labels:
        app: elasticsearch
        component: "client"
        release: R31E@S3
      annotations:
        checksum/config: e07f3415e40651891b0ed4f053eb1a781a7adcf74b381cb292eb3a827aaddbf5
    spec:
      serviceAccountName: R31E@S3-elasticsearch-client
      securityContext:
        fsGroup: 1000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: "elasticsearch"
                  release: "R31E@S3"
                  component: "client"
      initContainers:
      # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
      # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall
      - name: "sysctl"
        image: "busybox:latest"
        imagePullPolicy: "Always"
        resources:
            {}
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      containers:
      - name: elasticsearch
        env:
        - name: NODE_DATA
          value: "false"
        - name: NODE_MASTER
          value: "false"
        - name: DISCOVERY_SERVICE
          value: R31E@S3-elasticsearch-discovery
        - name: PROCESSORS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        - name: ES_JAVA_OPTS
          value: "-Djava.net.preferIPv4Stack=true -Xms512m -Xmx512m  "
        - name: MINIMUM_MASTER_NODES
          value: "2"
        resources:
            limits:
              cpu: "1"
            requests:
              cpu: 25m
              memory: 512Mi
        readinessProbe:
          httpGet:
            path: /_cluster/health
            port: 9200
          initialDelaySeconds: 5
        livenessProbe:
          httpGet:
            path: /_cluster/health?local=true
            port: 9200
          initialDelaySeconds: 90
        image: "docker.elastic.co/elasticsearch/elasticsearch-oss:6.5.4"
        imagePullPolicy: "IfNotPresent"
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        volumeMounts:
        - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
          name: config
          subPath: elasticsearch.yml
      volumes:
      - name: config
        configMap:
          name: R31E@S3-elasticsearch
---
# Source: graylog/charts/elasticsearch/templates/data-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.32.2
    component: "data"
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-elasticsearch-data
spec:
  selector:
    matchLabels:
      app: elasticsearch
      component: "data"
      release: R31E@S3
      role: data
  serviceName: R31E@S3-elasticsearch-data
  replicas: e1@st1cD@t@R3p1ic@
  template:
    metadata:
      labels:
        app: elasticsearch
        component: "data"
        release: R31E@S3
        role: data
    spec:
      serviceAccountName: R31E@S3-elasticsearch-data
      securityContext:
        fsGroup: 1000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: "elasticsearch"
                  release: "R31E@S3"
                  component: "data"
      initContainers:
      # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
      # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall
      - name: "sysctl"
        image: "busybox:latest"
        imagePullPolicy: "Always"
        resources:
            {}
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      - name: "chown"
        image: "docker.elastic.co/elasticsearch/elasticsearch-oss:6.5.4"
        imagePullPolicy: "IfNotPresent"
        resources:
            {}
        command:
        - /bin/bash
        - -c
        - >
          set -e;
          set -x;
          chown elasticsearch:elasticsearch /usr/share/elasticsearch/data;
          for datadir in $(find /usr/share/elasticsearch/data -mindepth 1 -maxdepth 1 -not -name ".snapshot"); do
            chown -R elasticsearch:elasticsearch $datadir;
          done;
          chown elasticsearch:elasticsearch /usr/share/elasticsearch/logs;
          for logfile in $(find /usr/share/elasticsearch/logs -mindepth 1 -maxdepth 1 -not -name ".snapshot"); do
            chown -R elasticsearch:elasticsearch $logfile;
          done
        securityContext:
          runAsUser: 0
        volumeMounts:
        - mountPath: /usr/share/elasticsearch/data
          name: data
      containers:
      - name: elasticsearch
        env:
        - name: DISCOVERY_SERVICE
          value: R31E@S3-elasticsearch-discovery
        - name: NODE_MASTER
          value: "false"
        - name: PROCESSORS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        - name: ES_JAVA_OPTS
          value: "-Djava.net.preferIPv4Stack=true -Xms1536m -Xmx1536m  "
        - name: MINIMUM_MASTER_NODES
          value: "2"
        image: "docker.elastic.co/elasticsearch/elasticsearch-oss:6.5.4"
        imagePullPolicy: "IfNotPresent"
        ports:
        - containerPort: 9300
          name: transport

        resources:
            limits:
              cpu: "1"
            requests:
              cpu: 25m
              memory: 1536Mi
        readinessProbe:
          httpGet:
            path: /_cluster/health?local=true
            port: 9200
          initialDelaySeconds: 5
        volumeMounts:
        - mountPath: /usr/share/elasticsearch/data
          name: data
        - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
          name: config
          subPath: elasticsearch.yml
        - name: config
          mountPath: /data-pre-stop-hook.sh
          subPath: data-pre-stop-hook.sh
        - name: config
          mountPath: /data-post-start-hook.sh
          subPath: data-post-start-hook.sh
        lifecycle:
          preStop:
            exec:
              command: ["/bin/bash","/data-pre-stop-hook.sh"]
          postStart:
            exec:
              command: ["/bin/bash","/data-post-start-hook.sh"]
      terminationGracePeriodSeconds: 3600
      volumes:
      - name: config
        configMap:
          name: R31E@S3-elasticsearch
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: OnDelete
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
        - "ReadWriteOnce"
      storageClassName: "St0r3@g3C1@ss"
      resources:
        requests:
          storage: "E1@st1cD@t@St0r@g3"
---
# Source: graylog/charts/elasticsearch/templates/master-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: elasticsearch
    chart: elasticsearch-1.32.2
    component: "master"
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-elasticsearch-master
spec:
  selector:
    matchLabels:
      app: elasticsearch
      component: "master"
      release: R31E@S3
      role: master
  serviceName: R31E@S3-elasticsearch-master
  replicas: e1@st1cM@st3rR3p1ic@
  template:
    metadata:
      labels:
        app: elasticsearch
        component: "master"
        release: R31E@S3
        role: master
    spec:
      serviceAccountName: R31E@S3-elasticsearch-master
      securityContext:
        fsGroup: 1000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: kubernetes.io/hostname
              labelSelector:
                matchLabels:
                  app: "elasticsearch"
                  release: "R31E@S3"
                  component: "master"
      initContainers:
      # see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
      # and https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#mlockall
      - name: "sysctl"
        image: "busybox:latest"
        imagePullPolicy: "Always"
        resources:
            {}
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      - name: "chown"
        image: "docker.elastic.co/elasticsearch/elasticsearch-oss:6.5.4"
        imagePullPolicy: "IfNotPresent"
        resources:
            {}
        command:
        - /bin/bash
        - -c
        - >
          set -e;
          set -x;
          chown elasticsearch:elasticsearch /usr/share/elasticsearch/data;
          for datadir in $(find /usr/share/elasticsearch/data -mindepth 1 -maxdepth 1 -not -name ".snapshot"); do
            chown -R elasticsearch:elasticsearch $datadir;
          done;
          chown elasticsearch:elasticsearch /usr/share/elasticsearch/logs;
          for logfile in $(find /usr/share/elasticsearch/logs -mindepth 1 -maxdepth 1 -not -name ".snapshot"); do
            chown -R elasticsearch:elasticsearch $logfile;
          done
        securityContext:
          runAsUser: 0
        volumeMounts:
        - mountPath: /usr/share/elasticsearch/data
          name: data
      containers:
      - name: elasticsearch
        env:
        - name: NODE_DATA
          value: "false"
        - name: DISCOVERY_SERVICE
          value: R31E@S3-elasticsearch-discovery
        - name: PROCESSORS
          valueFrom:
            resourceFieldRef:
              resource: limits.cpu
        - name: ES_JAVA_OPTS
          value: "-Djava.net.preferIPv4Stack=true -Xms512m -Xmx512m  "
        - name: MINIMUM_MASTER_NODES
          value: "2"
        resources:
            limits:
              cpu: "1"
            requests:
              cpu: 25m
              memory: 512Mi
        readinessProbe:
          httpGet:
            path: /_cluster/health?local=true
            port: 9200
          initialDelaySeconds: 5
        image: "docker.elastic.co/elasticsearch/elasticsearch-oss:6.5.4"
        imagePullPolicy: "IfNotPresent"
        ports:
        - containerPort: 9300
          name: transport

        volumeMounts:
        - mountPath: /usr/share/elasticsearch/data
          name: data
        - mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
          name: config
          subPath: elasticsearch.yml
      volumes:
      - name: config
        configMap:
          name: R31E@S3-elasticsearch
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: OnDelete
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
        - "ReadWriteOnce"
      storageClassName: "St0r3@g3C1@ss"
      resources:
        requests:
          storage: "E1@st1cM@st3rSt0r@g3"
---
# Source: graylog/charts/mongodb-replicaset/templates/mongodb-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: mongodb-replicaset
    chart: mongodb-replicaset-3.11.2
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-mongodb-replicaset
spec:
  selector:
    matchLabels:
      app: mongodb-replicaset
      release: R31E@S3
  serviceName: R31E@S3-mongodb-replicaset
  replicas: M0ng0M@st3rR3p1ic@
  template:
    metadata:
      labels:
        app: mongodb-replicaset
        release: R31E@S3
      annotations:
        checksum/config: e2b9d527f49227f7896c6e51ec4263cfa8186ae966231beb4f23ea1b50cf35cb
    spec:
      securityContext:
        runAsUser: 999
        fsGroup: 999
        runAsNonRoot: true
      terminationGracePeriodSeconds: 30
      initContainers:
        - name: copy-config
          image: "busybox:1.29.3"
          imagePullPolicy: "IfNotPresent"
          command:
            - "sh"
          args:
            - "-c"
            - |
              set -e
              set -x

              cp /configdb-readonly/mongod.conf /data/configdb/mongod.conf
          volumeMounts:
            - name: workdir
              mountPath: /work-dir
            - name: config
              mountPath: /configdb-readonly
            - name: configdir
              mountPath: /data/configdb
          resources:
            {}
        - name: install
          image: "unguiculus/mongodb-install:0.7"
          args:
            - --work-dir=/work-dir
          imagePullPolicy: "IfNotPresent"
          volumeMounts:
            - name: workdir
              mountPath: /work-dir
          resources:
            {}
        - name: bootstrap
          image: "mongo:3.6"
          command:
            - /work-dir/peer-finder
          args:
            - -on-start=/init/on-start.sh
            - "-service=R31E@S3-mongodb-replicaset"
          imagePullPolicy: "IfNotPresent"
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: REPLICA_SET
              value: rs0
            - name: TIMEOUT
              value: "900"
            - name: SKIP_INIT
              value: "false"
            - name: TLS_MODE
              value: requireSSL
          volumeMounts:
            - name: workdir
              mountPath: /work-dir
            - name: init
              mountPath: /init
            - name: configdir
              mountPath: /data/configdb
            - name: datadir
              mountPath: /data/db
          resources:
            {}
      containers:
        - name: mongodb-replicaset
          image: "mongo:3.6"
          imagePullPolicy: "IfNotPresent"
          ports:
            - name: mongodb
              containerPort: 27017
          resources:
            {}
          command:
            - mongod
          args:
            - --config=/data/configdb/mongod.conf
            - --dbpath=/data/db
            - --replSet=rs0
            - --port=27017
            - --bind_ip=0.0.0.0
          livenessProbe:
            exec:
              command:
                - mongo
                - --eval
                - "db.adminCommand('ping')"
            initialDelaySeconds: 30
            timeoutSeconds: 5
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
          readinessProbe:
            exec:
              command:
                - mongo
                - --eval
                - "db.adminCommand('ping')"
            initialDelaySeconds: 5
            timeoutSeconds: 1
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
          volumeMounts:
            - name: datadir
              mountPath: /data/db
            - name: configdir
              mountPath: /data/configdb
            - name: workdir
              mountPath: /work-dir

      volumes:
        - name: config
          configMap:
            name: R31E@S3-mongodb-replicaset-mongodb
        - name: init
          configMap:
            defaultMode: 0755
            name: R31E@S3-mongodb-replicaset-init
        - name: workdir
          emptyDir: {}
        - name: configdir
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: datadir
        annotations:
      spec:
        accessModes:
          - "ReadWriteOnce"
        storageClassName: "St0r3@g3C1@ss"
        resources:
          requests:
            storage: "M0ng0DBSt0r@g3S1z3"
---
# Source: graylog/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: R31E@S3-graylog
  labels:
    helm.sh/chart: graylog-1.6.10
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.1"
spec:
  serviceName: R31E@S3-graylog
  replicas: Gr@yL0gM@st3rR3p1ic@
  selector:
    matchLabels:
      app.kubernetes.io/name: graylog
      app.kubernetes.io/instance: "R31E@S3"
      app.kubernetes.io/managed-by: "Helm"
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: graylog-1.6.10
        app.kubernetes.io/name: graylog
        app.kubernetes.io/instance: "R31E@S3"
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/version: "3.1"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9000"
    spec:
      serviceAccountName: R31E@S3-graylog
      initContainers:
        - name: "setup"
          image: "alpine"
          imagePullPolicy: "IfNotPresent"
          # Graylog journal will recursive in every subdirectories. Any invalid format directories will cause errors
          command:
            - /bin/sh
            - -c
            - |
              rm -rf /usr/share/graylog/data/journal/lost+found
              wget https://storage.googleapis.com/kubernetes-release/release/v1.16.3/bin/linux/amd64/kubectl -O /k8s/kubectl
              chmod +x /k8s/kubectl

              GRAYLOG_HOME=/usr/share/graylog
              chown -R 1100:1100 ${GRAYLOG_HOME}/data/
          env:
          volumeMounts:
            - name: journal
              mountPath: /usr/share/graylog/data/journal
            - mountPath: /k8s
              name: kubectl
      containers:
        - name: graylog-server
          image: "graylog/graylog:3.1"
          imagePullPolicy: "IfNotPresent"
          command:
            - /entrypoint.sh
          env:
            - name: GRAYLOG_SERVER_JAVA_OPTS
              value: "-Djava.net.preferIPv4Stack=true -XX:NewRatio=1 -server -XX:+ResizeTLAB -XX:+UseConcMarkSweepGC -XX:+CMSConcurrentMTEnabled -XX:+CMSClassUnloadingEnabled -XX:+UseParNewGC -XX:-OmitStackTraceInFastThrow -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap "
            - name: GRAYLOG_PASSWORD_SECRET
              valueFrom:
                secretKeyRef:
                  name: R31E@S3-graylog
                  key: graylog-password-secret
            - name: GRAYLOG_ROOT_PASSWORD_SHA2
              valueFrom:
                secretKeyRef:
                  name: R31E@S3-graylog
                  key: graylog-password-sha2
          securityContext:
            privileged: false
          ports:
            - containerPort: 9000
              name: graylog
          resources:
            limits:
              cpu: "1"
            requests:
              cpu: 500m
              memory: 1024Mi
          livenessProbe:
            httpGet:
              path: /api/system/lbstatus
              port: 9000
            initialDelaySeconds: 120
            periodSeconds: 30
            failureThreshold: 3
            successThreshold: 1
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /api/system/lbstatus
              port: 9000
            initialDelaySeconds: 60
            periodSeconds: 10
            failureThreshold: 3
            successThreshold: 1
            timeoutSeconds: 5
          volumeMounts:
            - name: journal
              mountPath: /usr/share/graylog/data/journal
            - name: config
              mountPath: /usr/share/graylog/config
            - name: entrypoint
              mountPath: /entrypoint.sh
              subPath: entrypoint.sh
            - name: kubectl
              mountPath: /k8s
          lifecycle:
            preStop:
              exec:
                command:
                  - bash
                  - -ec
                  - |
                    curl  -XPOST -sS \
                      -u "admin:${GRAYLOG_PASSWORD_SECRET}" \
                      -H "X-Requested-By: R31E@S3-graylog" \
                      http://localhost:9000/api/system/shutdown/shutdown
      terminationGracePeriodSeconds: 120
      volumes:
        - name: config
          configMap:
            name: R31E@S3-graylog
            items:
              - key: graylog.conf
                path: graylog.conf
                mode: 292 # 0444
              - key: log4j2.xml
                path: log4j2.xml
                mode: 292 # 0444
        - name: entrypoint
          configMap:
            name: R31E@S3-graylog
            items:
              - key: entrypoint.sh
                path: entrypoint.sh
                mode: 365 # 0555
        - name: kubectl
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: journal
      spec:
        accessModes:
          - "ReadWriteOnce"
        storageClassName: "St0r3@g3C1@ss"
        resources:
          requests:
            storage: "Gr@yL0gStor@g3"
---
# Source: graylog/templates/ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  annotations:
  labels:
    helm.sh/chart: graylog-1.6.10
    app.kubernetes.io/name: graylog
    app.kubernetes.io/instance: "R31E@S3"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/version: "3.1"
    app.kubernetes.io/component: "web"
  name: R31E@S3-graylog-web
spec:
  rules:
    - host: Ingr3ssH0stN@m3
      http:
        paths:
          
          - backend:
              serviceName: R31E@S3-graylog-web
              servicePort: graylog
---
# Source: graylog/charts/elasticsearch/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: R31E@S3-elasticsearch-test
  labels:
    app: R31E@S3-elasticsearch
    chart: "elasticsearch-1.32.2"
    heritage: "Helm"
    release: "R31E@S3"
  annotations:
    "helm.sh/hook": test-success
spec:
  initContainers:
    - name: test-framework
      image: "dduportal/bats:0.4.0"
      command:
      - "bash"
      - "-c"
      - |
        set -ex
        # copy bats to tools dir
        cp -R /usr/local/libexec/ /tools/bats/
      volumeMounts:
      - mountPath: /tools
        name: tools
  containers:
    - name: R31E@S3-test
      image: "dduportal/bats:0.4.0"
      command: ["/tools/bats/bats", "-t", "/tests/run.sh"]
      volumeMounts:
      - mountPath: /tests
        name: tests
        readOnly: true
      - mountPath: /tools
        name: tools
  volumes:
  - name: tests
    configMap:
      name: R31E@S3-elasticsearch-test
  - name: tools
    emptyDir: {}
  restartPolicy: Never
---
# Source: graylog/charts/mongodb-replicaset/templates/tests/mongodb-up-test-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: mongodb-replicaset
    chart: mongodb-replicaset-3.11.2
    heritage: Helm
    release: R31E@S3
  name: R31E@S3-mongodb-replicaset-test
  annotations:
    "helm.sh/hook": test-success
spec:
  initContainers:
    - name: test-framework
      image: dduportal/bats:0.4.0
      command:
        - bash
        - -c
        - |
          set -ex
          # copy bats to tools dir
          cp -R /usr/local/libexec/ /tools/bats/
      volumeMounts:
        - name: tools
          mountPath: /tools
  containers:
    - name: mongo
      image: "mongo:3.6"
      command:
        - /tools/bats/bats
        - -t
        - /tests/mongodb-up-test.sh
      env:
        - name: FULL_NAME
          value: R31E@S3-mongodb-replicaset
        - name: NAMESPACE
          value: N@m3Sp@c3
        - name: REPLICAS
          value: "3"
      volumeMounts:
        - name: tools
          mountPath: /tools
        - name: tests
          mountPath: /tests
  volumes:
    - name: tools
      emptyDir: {}
    - name: tests
      configMap:
        name: R31E@S3-mongodb-replicaset-tests
  restartPolicy: Never
